version: '3'
services:
  # Hadoop Namenode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    environment:
      - CLUSTER_NAME=datamaster
      - HADOOP_OPTS=-javaagent:/jmx_exporter/jmx_prometheus_javaagent.jar=7074:/jmx_exporter/hadoop-jmx-config.yaml
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
      - ./configs/hadoop:/etc/hadoop
      - ./configs/jmx_exporter:/jmx_exporter/
    ports:
      - "9870:9870" # HDFS Web UI
      - "8020:8020" # HDFS Namenode
      - "7074:7074" # JMX Exporter port
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - hadoop-datanode

  # Hadoop Datanode
  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      - HADOOP_OPTS=-javaagent:/jmx_exporter/jmx_prometheus_javaagent.jar=7075:/jmx_exporter/hadoop-jmx-config.yaml
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
      - ./configs/hadoop:/etc/hadoop
      - ./configs/jmx_exporter:/jmx_exporter
    ports:
        - "7075:7075" # JMX Exporter port
    networks:
      - hadoop-spark-kafka-net


  # Spark Master
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_JARS_PACKAGES=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
      - SPARK_HISTORY_OPTS=-Dspark.history.ui.port=18080
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - PATH=/spark/bin:$PATH
      - SPARK_CONF_spark.ui.prometheus.enabled=true
      - SPARK_CONF_spark.metrics.conf=/etc/metrics/spark-metrics.properties
      - SPARK_DAEMON_JAVA_OPTS=-javaagent:/jmx_exporter/jmx_prometheus_javaagent.jar=7072:/jmx_exporter/spark-jmx-config.yaml
    ports:
      - "8080:8080" # Spark Master UI
      - "7077:7077" # Spark Master
      - "7072:7072" # JMX Exporter port
    volumes:
      - ./configs/jmx_exporter:/jmx_exporter
      - ./configs/metrics.properties:/spark/conf/metrics.properties
      - ./configs/metrics:/etc/metrics
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - hadoop-namenode
      - hadoop-datanode

  # Spark Worker
  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_CONF_spark.hadoop.yarn.resourcemanager.address=hadoop-resourcemanager:8032
      - SPARK_CONF_spark.sql.catalogImplementation=hive
      - SPARK_CONF_spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
      - SPARK_DAEMON_JAVA_OPTS=-javaagent:/jmx_exporter/jmx_prometheus_javaagent.jar=7073:/jmx_exporter/spark-jmx-config.yaml
    ports:
      - "7073:7073" # JMX Exporter port
    volumes:
      - ./configs/jmx_exporter:/jmx_exporter
      - ./configs/metrics.properties:/spark/conf/metrics.properties
      - ./configs/metrics:/etc/metrics
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - spark-master


  # Kafka
  kafka:
    image: wurstmeister/kafka:2.12-2.5.0
    container_name: kafka
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OPTS: >
        -javaagent:/jmx_exporter/jmx_prometheus_javaagent.jar=7071:/jmx_exporter/kafka-jmx-config.yaml
    ports:
      - "9092:9092" # Porta do Kafka
      - "7071:7071" # JMX Exporter port
    volumes:
      - ./configs/jmx_exporter:/jmx_exporter
    depends_on:
      - zookeeper
    networks:
      - hadoop-spark-kafka-net

  # Zookeeper (Kafka dependency)
  zookeeper:
    image: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181" # Zookeeper client port
    networks:
      - hadoop-spark-kafka-net

  kafka-exporter:
    image: danielqsj/kafka-exporter
    container_name: kafka-exporter
    environment:
      - KAFKA_SERVER=kafka:9092 # Endereço do broker Kafka
      - KAFKA_EXPORTER_LOG_LEVEL=info
      - KAFKA_EXPORTER_GROUP_FILTER=.*
      - KAFKA_EXPORTER_TOPIC_FILTER=.*
      - KAFKA_EXPORTER_RETRY_BACKOFF="500ms"
      - KAFKA_EXPORTER_RETRY_MAX_BACKOFF="30s"
    ports:
      - "9308:9308" # Porta para expor métricas do Kafka Exporter
    depends_on:
      - kafka
    networks:
      - hadoop-spark-kafka-net
    volumes:
      - ./configs/wait-for-kafka-sh.sh:/wait-for-kafka.sh
    entrypoint: ["/bin/sh", "/wait-for-kafka.sh", "kafka_exporter"]

  # Jupyter Notebook with PySpark
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./notebooks:/home/jovyan/work # Monta um volume para notebooks locais
    ports:
      - "8888:8888" # Jupyter Notebook port
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - spark-master
    command: start-notebook.sh --NotebookApp.token=''

  # PostgreSQL Database for Airflow
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432" # PostgreSQL port
    networks:
      - hadoop-spark-kafka-net

  # Airflow Scheduler and Webserver
  airflow:
    build:
      context: .
      dockerfile: Dockerfile-aiflow-spark
    container_name: airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=9yJ4LSnGW8ZYpIk-ySm7x6Udva2G9MB2DveCCrqx6Jg=
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=True
      - AIRFLOW__ELASTICSEARCH_CONFIGS__VERIFY_CERTS=False
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags # Diretório para DAGS do Airflow
      - airflow_data:/airflow
      - ./configs/setup_connections.py:/opt/setup_connections.py
    ports:
      - "8082:8080" # Airflow Web UI
      - "8125:8125" # Metrics
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - postgres
      - spark-master
      - kafka
    command: >
      bash -c "
      airflow db migrate &&
      airflow connections create-default-connections &&
      airflow users create --username admin --password admin --firstname Admin --lastname Admin --role Admin --email admin@example.com &&
      python /opt/setup_connections.py &&
      airflow scheduler & sleep 5 && airflow webserver"

  # Produtor Kafka para consumir dados da API da Riot Games
  riot-kafka-summoner_details:
    build:
      context: .
      dockerfile: Dockerfile-riot-api
    container_name: riot-api-summoner-detail
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
    command: ["/app/wait-for-kafka.sh", "kafka", "python", "/app/kafka_summoner_details.py" ]
    networks:
      - hadoop-spark-kafka-net

  riot-kafka-matchs:
    build:
      context: .
      dockerfile: Dockerfile-riot-api
    container_name: riot-api-matchs
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
    command: ["/app/wait-for-kafka.sh", "kafka", "python", "/app/kafka_matchs.py" ]
    networks:
      - hadoop-spark-kafka-net

  spark-consumer-kafka-to-lake:
    build:
      context: .
      dockerfile: Dockerfile-riot-ingestion
    container_name: spark-consumer
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
      - spark-master
      - spark-worker
      - hadoop-datanode
      - hadoop-namenode
    networks:
      - hadoop-spark-kafka-net
    command: [ "/app/wait-for-kafka.sh", "kafka", "python", "/app/consumer_kafka_to_lake.py" ]

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090" # Prometheus UI
    networks:
      - hadoop-spark-kafka-net

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/grafana/dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
      - ./configs/grafana/Hadoop-1733763131072.json:/etc/grafana/provisioning/dashboards/Hadoop-1733763131072.json
      - ./configs/grafana/Kafka Metrics-1733763119004.json:/etc/grafana/provisioning/dashboards/Kafka Metrics-1733763119004.json
      - ./configs/grafana/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
    ports:
      - "3000:3000" # Grafana UI
    networks:
      - hadoop-spark-kafka-net
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus

volumes:
  hadoop-namenode:
  hadoop-datanode:
  airflow_data:
  postgres_data:
  grafana_data:
  es_data:

networks:
  hadoop-spark-kafka-net:
    driver: bridge
