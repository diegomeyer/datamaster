version: '3'
services:
  # Hadoop Namenode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    environment:
      - CLUSTER_NAME=datamaster
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
      - ./configs/hadoop:/etc/hadoop
    ports:
      - "9870:9870" # HDFS Web UI
      - "8020:8020" # HDFS Namenode
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - hadoop-datanode

  # Hadoop Datanode
  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
      - ./configs/hadoop:/etc/hadoop
    networks:
      - hadoop-spark-kafka-net


  # Spark Master
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_JARS_PACKAGES=org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
      - SPARK_HISTORY_OPTS=-Dspark.history.ui.port=18080
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - PATH=/spark/bin:$PATH
      - SPARK_JARS_PACKAGES=org.apache.spark:spark-hive_2.12:3.3.0
      - SPARK_CONF_spark.sql.catalogImplementation=hive
      - SPARK_CONF_spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
    ports:
      - "8080:8080" # Spark Master UI
      - "7077:7077" # Spark Master
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - hadoop-namenode
      - hadoop-datanode

  # Spark Worker
  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_CONF_spark.hadoop.yarn.resourcemanager.address=hadoop-resourcemanager:8032
      - SPARK_CONF_spark.sql.catalogImplementation=hive
      - SPARK_CONF_spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083

    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - spark-master


  # Kafka
  kafka:
    image: wurstmeister/kafka:2.12-2.5.0
    container_name: kafka
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092" # Porta do Kafka
    depends_on:
      - zookeeper
    networks:
      - hadoop-spark-kafka-net

  # Zookeeper (Kafka dependency)
  zookeeper:
    image: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181" # Zookeeper client port
    networks:
      - hadoop-spark-kafka-net

  # Jupyter Notebook with PySpark
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./notebooks:/home/jovyan/work # Monta um volume para notebooks locais
    ports:
      - "8888:8888" # Jupyter Notebook port
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - spark-master
    command: start-notebook.sh --NotebookApp.token=''

  # PostgreSQL Database for Airflow
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432" # PostgreSQL port
    networks:
      - hadoop-spark-kafka-net

  # Airflow Scheduler and Webserver
  airflow:
    build:
      context: .
      dockerfile: Dockerfile-aiflow-spark
    container_name: airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=9yJ4LSnGW8ZYpIk-ySm7x6Udva2G9MB2DveCCrqx6Jg=
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=True
      - AIRFLOW__ELASTICSEARCH_CONFIGS__VERIFY_CERTS=False
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags # DiretÃ³rio para DAGS do Airflow
      - airflow_data:/airflow
      - ./configs/setup_connections.py:/opt/setup_connections.py
    ports:
      - "8082:8080" # Airflow Web UI
      - "8125:8125" # Metrics
    networks:
      - hadoop-spark-kafka-net
    depends_on:
      - postgres
      - spark-master
      - kafka
    command: >
      bash -c "
      airflow db migrate &&
      airflow connections create-default-connections &&
      airflow users create --username admin --password admin --firstname Admin --lastname Admin --role Admin --email admin@example.com &&
      airflow scheduler &
      airflow webserver"

  # Produtor Kafka para consumir dados da API da Riot Games
  riot-kafka-summoner_details:
    build:
      context: .
      dockerfile: Dockerfile-riot-api
    container_name: riot-api-summoner-detail
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
    command: ["/app/wait-for-kafka.sh", "kafka", "python", "/app/kafka_summoner_details.py" ]
    networks:
      - hadoop-spark-kafka-net

  riot-kafka-matchs:
    build:
      context: .
      dockerfile: Dockerfile-riot-api
    container_name: riot-api-matchs
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
    command: ["/app/wait-for-kafka.sh", "kafka", "python", "/app/kafka_matchs.py" ]
    networks:
      - hadoop-spark-kafka-net

  spark-consumer-kafka-to-lake:
    build:
      context: .
      dockerfile: Dockerfile-riot-ingestion
    container_name: spark-consumer
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
      - spark-master
      - spark-worker
      - hadoop-datanode
      - hadoop-namenode
    networks:
      - hadoop-spark-kafka-net
    command: [ "/app/wait-for-kafka.sh", "kafka", "python", "/app/consumer_kafka_to_lake.py" ]

volumes:
  hadoop-namenode:
  hadoop-datanode:
  airflow_data:
  postgres_data:
  grafana_data:
  es_data:

networks:
  hadoop-spark-kafka-net:
    driver: bridge
